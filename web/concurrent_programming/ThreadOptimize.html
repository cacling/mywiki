<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link rel="stylesheet" href="../../web/css/common.css" type="text/css" charset="utf-8" />
<link rel="stylesheet" href="../js/highlight/styles/default.css">
<script type="text/javascript" src="../js/highlight/highlight.pack.js"></script>
<script>
  hljs.tabReplace = '    ';
  hljs.initHighlightingOnLoad();
</script>
  

<title>多线程优化</title>
</head>
<body>
<div id="container">

<div class="seg"><h2><a name="1.1">线程开销分析与锁优化</a></h2>

<h3>线程引入的开销分析</h3>
如果是单线程的时候，既然不会存在线程的调度，也就不存在同步的开销的问题。<br>
在多个线程的协调和调度必然需要一定的性能开销，所以有时候为了提升性能而引入多线程，反而开销的消耗超过提升的幅度。<br>
如果运行的线程数大于CPU的数量，那么操作系统会把某个正在运行的线程调度出来，从而使其它线程能使用CPU，这将导致一次上下文的切换。<br>
上下文的切换有两部分，一部分是操作系统和JVM的开销，另一部分是缓存重载时的开销，因为一个新线程被切换进来时，它所需要的数据可能不再当前处理器的本地缓存之中。<br>
因为线程切换开销的存在，所以即使有很多线程在等待执行，操作系统的调度器也会保证线程运行的最小执行时间以保证不会开销过大。当然，这会以损失一部分及时响应性作为代价。<br>
<br>

当数据竞争不可避免需要用到锁的时候，主要有两个方面影响到并发的性能：<br>
- 锁的持有时间<br>
- 锁请求的频率<br>
<br>

所以降低数据的竞争程度，增加并发性能的方式有：<br>
- 减少锁的持有时间<br>
- 降低锁的请求频率<br>
<br>

<h4>减少锁的持有时间</h4>
减少锁的持有时间能有效降低发生竞争的可能性。如果某个操作持有锁的时间超过2毫秒，而且所有的操作都需要这个锁，那么无论拥有多少个空闲处理器，吞吐量也不会超过每秒500个操作。<br>
如果将这个锁的持有时间降低为1毫秒，那个能够将这个锁对应的吞吐量提高到每秒1000个操作。<br>
<br>
所以最直观的做法就是将一些与锁无关的代码移除出临界区，尤其是开销大的操作，例如大量的计算操作，以及可能被阻塞的操作，例如I/O操作。<br>
如例子所示，通过缩小userLocationMathes方法中锁的作用范围，能减少在持有锁的时间，根据Amdahl定律，因为串行代码的量减少了，更有效的提高并行的效率。<br>
<pre><code>
public class AttributeStore {

	private final Map<String, String> attributes = new HashMap<String, String>();

	public synchronized boolean userLocationMathes(String name, String regexp) {
		String key = "users." + name + ".location";
		String location = attributes.get(key);
		if (location == null) {
			return false;
		} else {
			return Pattern.matches(regexp, location);
		}
	}

}
</code></pre>
<br>
<pre><code>
public class AttributeStore {

	private final Map<String, String> attributes = new HashMap<String, String>();

	public boolean userLocationMathes(String name, String regexp) {
		String key = "users." + name + ".location";
		String location;
		synchronized (this){
			location = attributes.get(key);
		}
		if (location == null) {
			return false;
		} else {
			return Pattern.matches(regexp, location);
		}
	}

}
</code></pre>

然而，缩小同步代码并不一定带来效率的提升，因为同步代码过小，就会分解为多个同步代码块，同步需要一定的开销，所以反而会对性能带来负面的影响。需要在代码块大小与数量间作一个平衡。<br>
<br>

锁消除<br>
<br>
通过减少线程切换的时间消除一些不必要的同步开销以减少锁的持有时间。<br>
例如去尽掉除掉没有必要的同步块<br>
<pre><code>
synchronized(new Object()) {
	//Do something here ......
}
</code></pre>
其实在JVM里面也比较智能，可以依据来源于逃逸分析的数据支持（逃逸分析技术），如果判断到一段代码中，在堆上的所有数据都不会逃逸出去被其他线程访问到，
那就可以把它们当作栈上数据对待，认为它们是线程私有的，同步加锁自然就无须进行。
但是，还是在复杂的情况下JVM并不能做到尽善尽美，所以尽量根据实际情况把没必要的同步块的去掉。<br>
<br>

锁粗化<br>
原则上，我们在编写代码的时候，推荐将同步块的作用范围限制得尽量小——只在共享数据的实际作用域中才进行同步，
这样是为了使得需要同步的操作数量尽可能变小，如果存在锁竞争，那等待锁的线程也能尽快地拿到锁。
但是如果一系列的连续操作都对同一个对象反复加锁和解锁，甚至加锁操作是出现在循环体中的，那即使没有线程竞争，
频繁地进行互斥同步操作也会导致不必要的性能损耗。<br>
锁将邻近的同步代码块用同一个锁合并起来<br>
<pre><code>
public String getNames() {
	List<String> names = new Vector<String>();
	names.add("Moe");
	names.add("Larry");
	names.add("Curly");
	return names.toString();
}
</code></pre>
在getNames方法中，把3个add与1个toString调用合并为单个锁操作以减少同步开销。<br>

<br>
自旋锁(Spin Lock)是通过通过循环不断地尝试获取锁，直到成功。而不是通过阻塞等待实现的。<br>
如果物理机器有一个以上的处理器，能让两个或以上的线程同时并行执行，我们就可以让后面请求锁的那个线程“稍等一会”，但不放弃处理器的执行时间，看看持有锁的线程是否很快就会释放锁。<br>
为了让线程等待，我们只须让线程执行一个忙循环（自旋），这项技术就是所谓的自旋锁。  <br>
自旋锁在JDK 1.4.2中就已经引入，只不过默认是关闭的，可以使用-XX:+UseSpinning参数来开启，在JDK 1.6中就已经改为默认开启了。<br>
如果锁被占用的时间很短，自旋等待的效果就会非常好<br>
自旋次数的默认值是10次，用户可以使用参数-XX:PreBlockSpin来尝试更改<br>
但是要记得，自旋等待等待并不一定 比挂起等待高效。这个效率的高低，取决于上下文切换的开销以及在成功获取锁之前需要等待的时间。<br>
如果等待的时间比较短，则适合采用自旋的方式，如果等待的时间长，那么自旋的线程只会白白消耗处理器资源，而不会做任何有用的工作，反而会带来性能的浪费，则适合采用线程挂起的方式。<br>
在JDK 1.6中引入了自适应的自旋锁。自适应意味着自旋的时间不再固定了，而是由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定。
如果在同一个锁对象上，自旋等待刚刚成功获得过锁，并且持有锁的线程正在运行中，那么虚拟机就会认为这次自旋也很有可能再次成功，进而它将允许自旋等待持续相对更长的时间，比如100个循环。
另一方面，如果对于某个锁，自旋很少成功获得过，那在以后要获取这个锁时将可能省略掉自旋过程，以避免浪费处理器资源。
有了自适应自旋，随着程序运行和性能监控信息的不断完善，虚拟机对程序锁的状况预测就会越来越准确，虚拟机就会变得越来越“聪明”了。<br>
<br>
其实JVM中对于非竞争同步带来的开销的优化已经很充分，所以我们应该把优化的重点放在那些真正发生锁竞争的地方。<br>
<br>
附注：<br>
轻量级锁 <br>
HotSpot虚拟机的对象头（Object Header）信息里包含锁标记。
在代码进入同步块的时候，如果此同步对象没有被锁定（锁标志位为“01”状态），虚拟机首先将在当前线程的栈帧中建立一个名为锁记录（Lock Record）的空间，用于存储锁对象目前的Mark Word的拷贝（官方把这份拷贝加了一个Displaced前缀，即Displaced Mark Word），
然后，虚拟机将使用CAS操作尝试将对象的Mark Word更新为指向Lock Record的指针。如果这个更新动作成功了，那么这个线程就拥有了该对象的锁，并且对象Mark Word的锁标志位（Mark Word的最后两个Bits）将转变为“00”，即表示此对象处于轻量级锁定状态，
如果这个更新操作失败了，虚拟机首先会检查对象的Mark Word是否指向当前线程的栈帧，如果是就说明当前线程已经拥有了这个对象的锁，那就可以直接进入同步块继续执行，否则说明这个锁对象已经被其他线程抢占了。
如果有两条以上的线程争用同一个锁，那轻量级锁就不再有效，要膨胀为重量级锁，锁标志的状态值变为“10”，Mark Word中存储的就是指向重量级锁（互斥量）的指针，后面等待锁的线程也要进入阻塞状态。 
轻量级锁能提升程序同步性能的依据是“对于绝大部分的锁，在整个同步周期内都是不存在竞争的”，这是一个经验数据。
如果没有竞争，轻量级锁使用CAS操作避免了使用互斥量的开销，但如果存在锁竞争，除了互斥量的开销外，还额外发生了CAS操作，因此在有竞争的情况下，轻量级锁会比传统的重量级锁更慢。 <br>
偏向锁<br>
Java 偏向锁(Biased Locking)是Java6引入的一项多线程优化。它通过消除资源无竞争情况下的同步原语，进一步提高了程序的运行性能。
轻量级锁也是一种多线程优化，它与偏向锁的区别在于，轻量级锁是通过CAS来避免进入开销较大的互斥操作，而偏向锁是在无竞争场景下完全消除同步，连CAS也不执行
（CAS本身仍旧是一种操作系统同步原语，始终要在JVM与OS之间来回，有一定的开销）。<br>
偏向锁，顾名思义，它会偏向于第一个访问锁的线程，如果在接下来的运行过程中，该锁没有被其他的线程访问，则持有偏向锁的线程将永远不需要触发同步。<br>
如果在运行过程中，遇到了其他线程抢占锁，则持有偏向锁的线程会被挂起，JVM会尝试消除它身上的偏向锁，将锁恢复到标准的轻量级锁。<br>
在JDK6中，偏向锁是默认启用的。它提高了单线程访问同步资源的性能。<br>
但试想一下，如果你的同步资源或代码一直都是多线程访问的，那么消除偏向锁这一步骤对你来说就是多余的。事实上，消除偏向锁的开销还是蛮大的。<br>
所以在你非常熟悉自己的代码前提下，大可禁用偏向锁 -XX:-UseBiasedLocking 。<br>
<br>
<br>
<br>
<br>



<h4>降低锁的请求频率</h4>
降低锁的请求频率可以通过锁分解和锁分段等技术来实现。这些技术中采用多个相互独立的锁来保护独立的状态变量，从而改变这些变量在之前由单个锁来保护的情况。<br>

<h5>锁分解</h5>
例如将锁的请求分布到更多的锁上，那么能有效地降低竞争程度。由于等待锁而被阻塞的线程会更少。<br>
这些技术能减少锁操作的粒度，然而锁越多，发生死锁的几率就越高。<br>
如下面例子，原先是用ServerStatus作为锁来保护用户状态和查询状态，在对锁进行分解后，每个新的细颗粒度锁上的访问量比原先一个锁的访问量少。<br>
如果在锁上的竞争并不激烈时，在性能和吞吐量等方面带来的提升将很有限。<br>
<pre><code>
public class ServerStatus {
	public final Set<String> users = new HashSet<String>();
	public final Set<String> queries = new HashSet<String>();
	public synchronized void addUser(String u) {
		users.add(u);
	}
	public synchronized void addQuery(String q) {
		queries.add(q);
	}
	public synchronized void removeUser(String u) {
		users.remove(u);
	}
	public synchronized void removeQuery(String q) {
		queries.remove(q);
	}
}
</code></pre>
<br>
<pre><code>
public class ServerStatus {
	public final Set<String> users = new HashSet<String>();
	public final Set<String> queries = new HashSet<String>();
	public  void addUser(String u) {
		synchronized(users){
			users.add(u);
		}
	}
	public void addQuery(String q) {
		synchronized(queries){
			queries.add(q);
		}
	}
	public void removeUser(String u) {
		synchronized(users){
			users.remove(u);
		}
	}
	public void removeQuery(String q) {
		synchronized(queries){
			queries.remove(q);
		}
	}
}
</code></pre>

<br>


<h5>锁分段</h5>
上面ServerStatus锁分解例子中，把一个竞争激烈的锁分解为两个锁，但缺不能进一步分解，这影响了可伸缩性的进一步提高。<br>
在某些情况下，可以将锁分解技术进一步扩展为对一组独立对象上的锁进行分解，这种情况叫锁分段。<br>
例如，在ConcurrentHashMap的实现中使用了一个包含16个锁的数组，每个锁保护所有散列桶的1/16，其中第N个散列桶由第（N mod 16）个锁来保护。假设散列函数具有合理的分布性，并且关键字能够实现均匀分布，那么这大约能把对于锁的请求减少到原来的1/16。<br>
<pre><code>
public class StripedMap {
	private static final int N_LOCKS = 16;
	private final Node[] buckets;
	private final Object[] locks;
	private static class Node {......}
	private final int hash(Object key){
		return Math.abs(key.hashCode() % buckets.length);
	}
	public Object get(Object key){
		int hash = hash(key);
		synchronized (locks[hash %N_LOCKS]){
			for(Node m = buckets[hash];m!=null;m=m.next())
				return m.value();
		}
		return null;
	}
	public void clear(){
		for(int i =0;i < buckets.length;i++){
			synchronized (locks[i % N_LOCKS]){
				buckets[i] = null;
			}
		}
	}
}
</code></pre>


</div>


<div class="seg"><h2><a name="1.2">关于多线程性能优化</a></h2>
《计算机程序设计的艺术》的作者Donald Knuth说的一番话：<br>
"过早的优化是万恶之源。"<br>
（洋洋数百万言的多卷本经典巨著 《计算机程序设计的艺术》(The Art of Computer Programming)堪称计算机科学理论与技术的经典巨著，有评论认为其作用与地位可与数学史上欧几里得的《几何学原理》相比。
本书作者唐纳德•克努特(Donald Ervin Knuth)因而荣获1974年度的图灵奖。 ）
<br>
1. 不要过早优化。在你的应用还没有遇到性能问题之前，不要为性能担忧，也无需为优化浪费时间，只有当性能成为问题时才进行优化。<br>
我们很多时候会把过早优化定义为以性能的名义使设计和代码更加复杂，更不可读，造成代码更难理解和操控，并需更大的精力进行维护。<br>
然而我们的这些努力并没有被需要的性能检验证明是恰当的（如实际的测量结果和与性能目标对照），将未经证实的价值加到程序中。时常，不必须的和未经测试的努力优化根本不能使程序更快。<br>
尤其是在系统构建初期，先别为是否使用本地方法，是否将某个方法声明为final，以及是否调整代码效率等问题而烦恼。<br>
衡量两次，优化一次。<br>
<br>
2. 基准测试是必须的。没有基准测试，优化的效果就无从衡量，所有优化的第一步都应该是基准测试。<br>
我们的主要目的应该是先证明设计的正确性。即使在设计本身也有效率需求的情况下，也要遵循“先能运行，再求快速”的准则。<br>
我们要先保证程序能正确的运行，得到一个基准的测试标准结果。当有证据证明某段代码确实存在性能瓶颈，再进行优化以求加快速度。<br>
<br>
3. 要适可而止，不要追求极致。<br>
我们知道托雷法则20/80法则，有20%的努力可以得到巨大的回报，而有80%努力得不到多少回报。<br>
性能的优化实际上是个没有尽头的事情，没有任何一个应用可以在性能方面达到完美，只要你愿意，总是能找到可以优化的地方，但是这里要进行权衡，
如果你的团队花了1个月的时间进行优化，而性能仅仅提高了10%，那么不如添置一台服务器，一台服务器的成本肯定要远远小于你的团队一个月的工资，
这并不是说代码优化毫无用处，但是你需要检查一下整个系统，并且确定把时间花在这上面是值得的。因为在优化代码上每花费一分钟，就意味着你少了增加新功能、编写文档或者编写单元测试的一分钟。<br>
另外，也没有必要为了一些可有可无的优化而牺牲代码的可读性，什么是可有可无的优化？比如直接写a标签肯定比link_to要快，调用 helper肯定也不如将所有代码写在view中快，但这样的优化得不偿失，没有意义。<br>
<br>
我们要尝试真正引起80%性能问题的那20%的地方，例如：<br>
- 是否存在特别慢或者导致可并行的工作停止或延误的程序段？例如：I/O经常是系统瓶颈。<br>
通过Admal定律，我们可以知道只要有串行部分存在，就会大幅度降低并行的效率。所以我们可以通过各种性能分析工具找出引起性能瓶颈的关键在于哪部分。<br>
判断瓶颈可以是使用各种profile工具，例如IBM的threaddump analyser，找出长期在等待或者是可能引起死锁的线程。<br>
或者通过Perf4j这样的一些三方软件，插入下述“显式”计时代码，量度一个特定任务的执行时间，对程序进行评测，找出最影响程序性能的瓶颈。<br>
<br>
- 识别程序的限制因素。常见的像Fibonacci数列中的那样的数据依赖的限制。看能否通过重构，或者使用不同的算法可以减少或消除程序中的瓶颈。<br>
<br>

</div>


</div>
</body>
</html>